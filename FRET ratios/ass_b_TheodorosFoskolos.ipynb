{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment B - RhoA activation and inhibition analysis using FRET\n",
    "### Author: Theodoros Foskolos\n",
    "### Date: June, 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-13T00:36:00.359958Z",
     "end_time": "2023-06-13T00:36:00.375452Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import napari\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy.ma as ma\n",
    "from scipy import ndimage\n",
    "from skimage import exposure, transform, io, img_as_ubyte\n",
    "import tifffile as tif\n",
    "from PIL import Image\n",
    "from skimage.registration import phase_cross_correlation\n",
    "from skimage.transform import AffineTransform, warp\n",
    "from scipy.ndimage import fourier_shift\n",
    "import imageio"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Import and inspect the image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-13T00:32:05.752178Z",
     "end_time": "2023-06-13T00:32:05.798406Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(70, 2, 260, 348)"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Start coding here\n",
    "filepath ='IPQDA_23_ASS_B_data/W47-SGFP2-mScarlet-I-01-1_2channels.tif'\n",
    "image = tif.imread(filepath)\n",
    "#End coding here\n",
    "image.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Extract both the donor and acceptor channels (donor = channel 1 and acceptor = channel 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-13T00:32:07.357923Z",
     "end_time": "2023-06-13T00:32:07.437302Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(70, 260, 348)"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start coding here\n",
    "donor_channel = image[:,0,:,:]\n",
    "acceptor_channel = image[:,1,:,:]\n",
    "donor_channel.shape\n",
    "# End coding here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Calclate and plot the z-projections (use mean) of both channels"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Question 4: What is the purpose of calculating the mean of the z-projection of each image stack: The purpose of calculating the Z-Projection of each image stack is to get the average intensity of the stacks as a single image representing all the Z axis. This procedure helps us detect the background ROI that we will subtract later and use it to reduce noise"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-13T00:32:08.749012Z",
     "end_time": "2023-06-13T00:32:08.793943Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(260, 348)"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start coding here\n",
    "z_project_donor = np.mean(donor_channel, axis=0)\n",
    "z_project_acceptor = np.mean(acceptor_channel, axis=0)\n",
    "z_project_acceptor.shape\n",
    "# End coding here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-12T18:32:57.135157Z",
     "end_time": "2023-06-12T18:32:58.971592Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#%matplotlib inline, if i use that it disturbs the  TkAGG\n",
    "# Plot z-projections\n",
    "plt.imshow(z_project_donor)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.imshow(z_project_acceptor)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-12T17:42:59.547170Z",
     "end_time": "2023-06-12T17:43:00.732209Z"
    }
   }
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract background mask (region with no cell(s))\n",
    "Refer to Tutorial 2 for steps to create mask of region of interest (ROI)\n",
    "\n",
    "I tried to do it based on the tutorial 2 by drawing the roi polylines on the background and use that as a mask but it seemed to miss something, and tutorial doesnt contain much information on the opencv method, only napari which i encountered several issues. Simple thresholding for both channels based on the mean of the mean projections worked fine and separated the background from the cells quite well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-12T19:42:15.665110Z",
     "end_time": "2023-06-12T19:42:15.681130Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"# Make donor and acceptor RGB format\n",
    "Donor_RGB = cv2.cvtColor(z_project_donor, cv2.COLOR_BGR2RGB)\n",
    "Acceptor_RGB = cv2.cvtColor(z_project_acceptor, cv2.COLOR_BGR2RGB)\n",
    "Donor_RGB.shape\n",
    "# End coding here\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"# Pick 3 points of the background to create a roi for donor and then for acceptor\n",
    "plt.imshow(Donor_RGB)\n",
    "points_donor = np.array(plt.ginput(4))\n",
    "plt.draw()\n",
    "plt.show()\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-12T18:37:31.500586Z",
     "end_time": "2023-06-12T18:37:45.513368Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"plt.imshow(Acceptor_RGB)\n",
    "points_accept = np.array(plt.ginput(4))\n",
    "plt.draw()\n",
    "plt.show()\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-12T18:37:47.209482Z",
     "end_time": "2023-06-12T18:37:51.801468Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"#Convert the selected points to integer coordinates:\n",
    "roi_points_donor =np.int32([points_donor])\n",
    "roi_points_accept = np.int32([points_accept])\"\"\"\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-12T18:37:52.919513Z",
     "end_time": "2023-06-12T18:37:52.938520Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"#Draw the ROI on the image using OpenCV:\n",
    "background_mask_Donor = cv2.polylines(Donor_RGB,roi_points_donor, isClosed=True, color=(0, 255, 0), thickness=1)\n",
    "background_mask_Accept = cv2.polylines(Acceptor_RGB,roi_points_accept, isClosed=True, color=(0, 255, 0), thickness=1)\"\"\"\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-12T18:37:53.369642Z",
     "end_time": "2023-06-12T18:37:53.387638Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"#Display the image with the ROI:\n",
    "plt.figure()\n",
    "plt.imshow(Acceptor_RGB)\n",
    "plt.show()\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-12T18:41:27.551303Z",
     "end_time": "2023-06-12T18:41:46.891646Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"#Display the image with the ROI:\n",
    "plt.figure()\n",
    "plt.imshow(Donor_RGB)\n",
    "plt.show()\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-12T18:37:56.997458Z",
     "end_time": "2023-06-12T18:37:59.128837Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "threshold_donor = np.mean(z_project_donor)\n",
    "threshold_acceptor = np.mean(z_project_acceptor)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T00:32:12.661384Z",
     "end_time": "2023-06-13T00:32:12.678442Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "# if intensity in both images is below the thresholds -> background = true\n",
    "background_mask_Donor = (z_project_donor < threshold_donor)\n",
    "background_mask_Accept = (z_project_acceptor < threshold_acceptor)\n",
    "#background_mask.shape\n",
    "plt.imshow(background_mask_Accept)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T00:32:13.160541Z",
     "end_time": "2023-06-13T00:32:15.066264Z"
    }
   }
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Calculate average background intensity and also the standard deviation for all time frames and both channels. Plot them vs time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-13T00:32:16.762882Z",
     "end_time": "2023-06-13T00:32:16.851201Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Mean and std dev calculation of the z projections background\n",
    "mean_donor_z = np.mean(background_mask_Donor)\n",
    "mean_acceptor_z = np.mean(background_mask_Accept)\n",
    "stddev_donor_z = np.std(background_mask_Donor)\n",
    "stddev_acceptor_z = np.std(background_mask_Accept)\n",
    "\n",
    "mean_donor = []\n",
    "mean_acceptor = []\n",
    "stddev_donor = []\n",
    "stddev_acceptor = []\n",
    "\n",
    "# Same, for each time point of the image in both channels based on the ROIs i selected before for the background.\n",
    "for t in range(image.shape[0]):\n",
    "    \"\"\" #This part is only for, ROI selection by hand, which seem to not work properly\n",
    "    # Extract the background region using the ROI points, also ensure that the selected points for the ROI are within the valid range of the acceptor channel image dimensions\n",
    "    background_donor = donor_channel[t][\n",
    "        np.clip(roi_points_donor[:, 1], 0, donor_channel.shape[1] - 1),\n",
    "        np.clip(roi_points_donor[:, 0], 0, donor_channel.shape[2] - 1)\n",
    "    ]\n",
    "    background_acceptor = acceptor_channel[t][\n",
    "        np.clip(roi_points_accept[:, 1], 0, acceptor_channel.shape[1] - 1),\n",
    "        np.clip(roi_points_accept[:, 0], 0, acceptor_channel.shape[2] - 1)\n",
    "    ]\"\"\"\n",
    "    # Get the background for frames\n",
    "    background_donor = donor_channel[t][background_mask_Donor]\n",
    "    background_acceptor = acceptor_channel[t][background_mask_Accept]\n",
    "\n",
    "    # Mean and standard deviation for the backgroundd and add it to the list for each timepoint\n",
    "    mean_donor.append(np.mean(background_donor))\n",
    "    mean_acceptor.append(np.mean(background_acceptor))\n",
    "    stddev_donor.append(np.std(background_donor))\n",
    "    stddev_acceptor.append(np.std(background_acceptor))\n",
    "\n",
    "# End coding here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-13T00:32:17.506526Z",
     "end_time": "2023-06-13T00:32:17.521049Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "\"# Plot average intensities of background and standard deviations against time\\ntime_frames = np.arange(donor_channel.shape[0])\\n\\nplt.figure(figsize=(10, 6))\\nplt.errorbar(time_frames, mean_donor, yerr=stddev_donor, label='Donor Channel')\\nplt.errorbar(time_frames, mean_acceptor, yerr=stddev_acceptor, label='Acceptor Channel')\\nplt.xlabel('Time')\\nplt.ylabel('Intensity')\\nplt.title('Average Intensity and Standard Deviation vs Time')\\nplt.legend()\\nplt.grid(True)\\nplt.show()\\n\\n\""
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# Plot average intensities of background and standard deviations against time\n",
    "time_frames = np.arange(donor_channel.shape[0])\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.errorbar(time_frames, mean_donor, yerr=stddev_donor, label='Donor Channel')\n",
    "plt.errorbar(time_frames, mean_acceptor, yerr=stddev_acceptor, label='Acceptor Channel')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Intensity')\n",
    "plt.title('Average Intensity and Standard Deviation vs Time')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "#Plotting\n",
    "plt.plot(mean_donor,label='donor')\n",
    "plt.plot(mean_acceptor,label='acceptor')\n",
    "# Set the labels\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('intensity')\n",
    "# Set the legend\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "plt.plot(stddev_donor,label='donor')\n",
    "plt.plot(stddev_acceptor,label='acceptor')\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('intensity')\n",
    "\n",
    "# Set the legend\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T00:32:18.104338Z",
     "end_time": "2023-06-13T00:32:21.150196Z"
    }
   }
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image processing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-13T00:32:22.333285Z",
     "end_time": "2023-06-13T00:32:22.377196Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Initializing parameters\n",
    "\n",
    "# Define a smoothing kernel\n",
    "ks = (3,3) # kernel size should be a tuple\n",
    "kernel = np.ones(ks) / np.prod(ks) # create box filter kernel\n",
    "\n",
    "# Initialize output array\n",
    "corrected_frames = np.zeros_like(image,dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "#Preparing the image data\n",
    "# Convert pixel type to float\n",
    "image = image.astype(np.float64)\n",
    "background_mask_Accept = background_mask_Accept.astype(np.float64)\n",
    "background_mask_Donor = background_mask_Donor.astype(np.float64)\n",
    "\n",
    "\"\"\" # this part also only for the hand picked ROI of background\n",
    "# I encountered this problem that the RGB has 3 channels and i can use it for subtraction and i dont know how so i make it gray\n",
    "mask_gray_don = cv2.cvtColor(background_mask_Donor, cv2.COLOR_RGB2GRAY)\n",
    "mask_gray_accept = cv2.cvtColor(background_mask_Accept, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "mask_gray_don.shape\n",
    "\n",
    "# Channels extraction\"\"\"\n",
    "\n",
    "donor_channel = image[:,0,:,:]\n",
    "acceptor_channel = image[:,1,:,:]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T00:32:22.815419Z",
     "end_time": "2023-06-13T00:32:22.858471Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-13T00:32:23.557388Z",
     "end_time": "2023-06-13T00:32:25.497860Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 of 70\n",
      "1 of 70\n",
      "2 of 70\n",
      "3 of 70\n",
      "4 of 70\n",
      "5 of 70\n",
      "6 of 70\n",
      "7 of 70\n",
      "8 of 70\n",
      "9 of 70\n",
      "10 of 70\n",
      "11 of 70\n",
      "12 of 70\n",
      "13 of 70\n",
      "14 of 70\n",
      "15 of 70\n",
      "16 of 70\n",
      "17 of 70\n",
      "18 of 70\n",
      "19 of 70\n",
      "20 of 70\n",
      "21 of 70\n",
      "22 of 70\n",
      "23 of 70\n",
      "24 of 70\n",
      "25 of 70\n",
      "26 of 70\n",
      "27 of 70\n",
      "28 of 70\n",
      "29 of 70\n",
      "30 of 70\n",
      "31 of 70\n",
      "32 of 70\n",
      "33 of 70\n",
      "34 of 70\n",
      "35 of 70\n",
      "36 of 70\n",
      "37 of 70\n",
      "38 of 70\n",
      "39 of 70\n",
      "40 of 70\n",
      "41 of 70\n",
      "42 of 70\n",
      "43 of 70\n",
      "44 of 70\n",
      "45 of 70\n",
      "46 of 70\n",
      "47 of 70\n",
      "48 of 70\n",
      "49 of 70\n",
      "50 of 70\n",
      "51 of 70\n",
      "52 of 70\n",
      "53 of 70\n",
      "54 of 70\n",
      "55 of 70\n",
      "56 of 70\n",
      "57 of 70\n",
      "58 of 70\n",
      "59 of 70\n",
      "60 of 70\n",
      "61 of 70\n",
      "62 of 70\n",
      "63 of 70\n",
      "64 of 70\n",
      "65 of 70\n",
      "66 of 70\n",
      "67 of 70\n",
      "68 of 70\n",
      "69 of 70\n"
     ]
    }
   ],
   "source": [
    "#Now they begin - image processing\n",
    "\n",
    "for t in range(image.shape[0]):\n",
    "    \n",
    "    #######################################\n",
    "    ##Background subtraction\n",
    "    #Start coding here\n",
    "    donor_channel[t, :, :] = donor_channel[t, :, :] - background_mask_Donor\n",
    "    acceptor_channel[t, :, :] = acceptor_channel[t, :, :] - background_mask_Accept\n",
    "\n",
    "    #######################################\n",
    "    ##Image registration\n",
    "    #Calculate the shift between the two images\n",
    "    shift, error, diffphase = phase_cross_correlation(donor_channel[t, :, :], acceptor_channel[t, :, :])\n",
    "\n",
    "\n",
    "    #Create an affine transform object with the shift\n",
    "    tform = AffineTransform(translation=(-shift[1], -shift[0]))\n",
    "\n",
    "    #Apply the transformation to the image \n",
    "    corrected_image = warp(acceptor_channel[t, :, :], tform.inverse)\n",
    "\n",
    "    # Replace the original channel with the corrected channel\n",
    "    acceptor_channel[t, :, :] = corrected_image\n",
    "\n",
    "    #######################################\n",
    "    ##Image processing steps to reduce noise\n",
    "    ##Smoothing (hint: use the created kernel)\n",
    "    donor_channel[t, :, :] = ndimage.convolve(donor_channel[t, :, :], kernel)\n",
    "    acceptor_channel[t, :, :] = ndimage.convolve(acceptor_channel[t, :, :], kernel)\n",
    "\n",
    "    #Thresholding so that pixels with low signal are assigned with np.nan. \n",
    "    #Use the background mean and standard deviation intensity in this step to form a threshold value\n",
    "    t_donor = mean_donor[t] +  stddev_donor[t]\n",
    "    t_acceptor = mean_acceptor[t] +  stddev_donor[t]\n",
    "    donor_channel[t, :, :][donor_channel[t, :, :] < t_donor] = np.nan\n",
    "    acceptor_channel[t, :, :][acceptor_channel[t, :, :] < t_acceptor] = np.nan\n",
    "    #Well done. Let's keep the corrected frames\n",
    "    corrected_frames[t, 1, :, :] = acceptor_channel[t, :, :]\n",
    "    corrected_frames[t, 0, :, :] = donor_channel[t, :, :]\n",
    "    print(f'{t} of {image.shape[0]}')\n",
    "        \n",
    "\n",
    "    # End coding here"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Question 5 : The threshold value is used to distinguish between pixels with high signal, which reflect the intended characteristics or objects in the image, and pixels with low signal, which are more likely to be influenced by noise. We can determine a cutoff point below which the pixel values are regarded as noise or background by establishing a threshold based on the background mean and standard deviation intensities. The background standard deviation intensity shows the variety or spread of intensity values in the background, whereas the background mean intensity estimates the average intensity level in the background region. In order to denote insignificance or noise, pixels having intensity levels below a particular threshold (ex. threshold = mean_donor[t] + stddev_donor[t]. This means that the threshold is set as the mean plus the standard deviation) are presumed to be part of the background and are given the value np.NAN. In general this procedure helps us  reduce noise in the subsequent analysis, focusing on the meaningful signal (our 4 cells) and getting quality results."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Question 6 : In the line with the AffineTransform function, we create an object with a translation transformation that shifts the image in the opposite direction by the specified values, in general based on skimage it allows you to define various types of transformations including translation, rotation, scaling, and shearing."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-13T01:51:10.872943Z",
     "end_time": "2023-06-13T01:51:15.451440Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# PLot\n",
    "plt.imshow(corrected_frames[12, 0,:,:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-13T00:32:28.919519Z",
     "end_time": "2023-06-13T00:32:30.689545Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate FRET ratios. R = Acceptor/Donor\n",
    "# Start coding here\n",
    "#ratio =  acceptor_channel / donor_channel\n",
    "ratio =  corrected_frames[:, 1,:,:] / corrected_frames[:, 0,:,:]\n",
    "\n",
    "# End coding here\n",
    "\n",
    "#Plot a layer\n",
    "plt.imshow(ratio[2,:,:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "# FRET ratio across all pixels for each time-point, ignoring the NAN values\n",
    "mean_ratio = np.nanmean(ratio, axis=(1, 2))\n",
    "\n",
    "# Plot the mean FRET ratio over time\n",
    "plt.plot(mean_ratio)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Mean FRET Ratio')\n",
    "plt.title('Mean FRET Ratio Over Time')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-13T01:22:42.239445Z",
     "end_time": "2023-06-13T01:23:18.761923Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Apply LUT, transform the image to 8bit and export as Gif"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-13T00:36:37.943922Z",
     "end_time": "2023-06-13T00:37:01.807655Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the custom LUT\n",
    "# Hot LUT:\n",
    "lut_hot = np.zeros((256, 3), dtype=np.uint8)\n",
    "lut_hot[:, 0] = np.arange(256)\n",
    "lut_hot[:, 1] = np.arange(256) // 2\n",
    "lut_hot[:, 2] = 0\n",
    "\n",
    "\n",
    "# Create a list to store frames of FRET ratio images\n",
    "frames = []\n",
    "\n",
    "# Iterate over the FRET ratio images and convert them to PIL Image objects\n",
    "for i in range(len(ratio)):\n",
    "    # Convert the FRET ratio image to 8-bit using img_as_ubyte\n",
    "    ratio_8bit = img_as_ubyte(ratio[i])\n",
    "\n",
    "    # Apply the custom LUT to the 8-bit image\n",
    "    ratio_lut = lut_hot[ratio_8bit]\n",
    "\n",
    "    # Convert the LUT image array to a PIL Image object\n",
    "    ratio_image = Image.fromarray(ratio_lut)\n",
    "\n",
    "    # Create a new figure for each frame\n",
    "    plt.figure()\n",
    "\n",
    "    # Show the FRET ratio image\n",
    "    plt.imshow(ratio_image, cmap='gray')\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Add frame number and text annotations\n",
    "    plt.text(20, 20, f'Frame {i}', color='white', fontsize=5, weight='bold', backgroundcolor='black')\n",
    "\n",
    "    if i >= 15 and i < 45:\n",
    "        plt.text(10, 10, 'Histamine Activation', color='white', fontsize=8, weight='bold', backgroundcolor='black')\n",
    "    elif i >= 45:\n",
    "        plt.text(10, 10, 'Mepyramine Inhibition', color='white', fontsize=8, weight='bold', backgroundcolor='black')\n",
    "\n",
    "    # Save the figure\n",
    "    plt.savefig('tmp.png', bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()\n",
    "\n",
    "    # Open the temporary image and convert it to a PIL Image object\n",
    "    temp_image = Image.open('tmp.png')\n",
    "\n",
    "    # Append the PIL Image object to the frames list\n",
    "    frames.append(np.array(temp_image))\n",
    "\n",
    "# file path\n",
    "output_path = \"fret_ratios.gif\"\n",
    "\n",
    "# Save the frames as an animated GIF\n",
    "imageio.mimsave(output_path, frames, duration=200, loop=0)\n",
    "os.remove(\"tmp.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
